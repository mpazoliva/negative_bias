# -*- coding: utf-8 -*-
"""NegativeBias (Version 2) . ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RWGgRhmNUpTdJ-lCECWBsWPAYYe9O8_3

# **Cognitive bias in depression and anxiety: NLP analysis.**

Team: Satrajit S. Ghosh, Daniel M. Low, Maria Paz Oliva, Fernando Torrente.

This work is based on a survey in which participants were asked about future events during COVID-19 confinement. Measures related to depression and anxiety of the subjects were taken. Using NLP techniques and ML, we expect to be able predict the levels of depression and anxiety of the participants from their speeches.

# *1 - LOAD DATA SET*

The following dataset was build based on a survey carried out by INECO.
3617 participants were asked about future events during the beginning of the COVID-19 confinement.
Measures related to the depression, anxiety and other behaivors of the subjects were taken.
The data set has the following columns of information: 

1- Depression level ( based on PHQ-9: 0 to 27)

2- Anxiety level (based on GAD-7: 0 to 21)

3- Mental fatigue level ( 0 to 5) check!!!!

4- Risk perception ( 0 to 10)

5- Lockdown adherence (0 to 10)

6- Loneliness (0 to 10)

7- Intolerance to uncertainty ( 0 to 5) check!!!  

8- Answer to the question: “¿Cómo te imaginas que cambiará tu vida después de 
la pandemia?

9- Answer to the question:  “¿Cómo te imaginás que podría ser tu próximo verano?"

## Upload and clean data-set.
"""

# Mount google drive
from google.colab import drive
drive.mount('/content/drive')

# Load data and create data frame

# 9 columns: depression, anxiaty, mental fatigue, risk perception, lockdown adherence, intolerance to uncertainty, post pandemic answer, next summer answer
# 3.617 rows 

import pandas as pd 
# Set colum names
names = ['depression', 'anxiety', 'mental_fatigue', 'risk_perception', 'lockdown_adherence', 'loneliness', 'intolerance_uncertainty', 'post_pandemic', 'next_summer' ]
# Load csv file
df = pd.read_csv('/content/drive/MyDrive/NEGATIVE BIAS/DATA SETS/dataset-negativebias - Hoja 1.csv', names=names)
# Remove Nan answers
df = df.dropna()
# Viasualization
print("Df shape is:", df.shape)
print(df)

"""## Descriptive statistics

Summary of the values obtained for the following columns: 

1- Depression level ( 0 to 27)

2- Anxiety level ( 0 to 21)

3- Mental fatigue level ( 0 to 5) CHECK: 5 TO 25?

4- Risk perception ( 0 to 10) 

5- Lockdown adherence (0 to 10)

6- Loneliness (0 to 10)

7- Intolerance to uncertainty ( 0 to 5) CHECK: 12 to 60?
"""

#Summary of the statistics 
df.describe().round(1).T

"""# *2- DEFINE PRE-PROCESSING FUNCTION*

This pre-process intends to perform: lower case, remove punctuation, remove stop words and tokenization. 

The output is a list of tokens for each doc.

Check: did not perform lemmatization because it was not working well.
"""

import string
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer

def preprocess(df):
    # Lowercase text
    df["post_pandemic"] = df["post_pandemic"].str.lower()
    df["next_summer"] = df["next_summer"].str.lower()

    # Remove punctuation
    translator = str.maketrans('', '', string.punctuation)
    df["post_pandemic"] = df["post_pandemic"].str.translate(translator)
    df["next_summer"] = df["next_summer"].str.translate(translator)

    # Remove stopwords for Spanish
    stopwords_es = set(stopwords.words('spanish'))
    df["post_pandemic"] = df["post_pandemic"].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords_es]))
    df["next_summer"] = df["next_summer"].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords_es]))

    # Lemmatize in Spanish
    #stemmer = SnowballStemmer('spanish')
    #df["post_pandemic"] = df["post_pandemic"].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))
    #df["next_summer"] = df["next_summer"].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))

    # Tokenize
    df["post_pandemic"] = df["post_pandemic"].apply(lambda x: word_tokenize(x))
    df["next_summer"] = df["next_summer"].apply(lambda x: word_tokenize(x))

    return df

# Preprocess df
preprocess(df)

"""# 3 - *FEATURES EXTRACTION*

The following items will be computed to be used as features and added to the data set: 

1- Sentiment analysis: determine the attitude that the subjects have regarding their own future, establishing a gradual scale whose poles are: positive attitude and negative attitude.

2- Word count: count the amount of words used by each subject.

3- Lexicons: design lexical sets and count their appearance.

4- Concreteness: by having a concreteness value for each token, we can take the average concreteness of a document (removing the tokens that are not in the dictionary from the count).

## 1) SENTIMENT ANALYSIS

To perform sentiment analysis we will use the pipeline function from transformers. 

The classifier is a pre-trained bert model: "pysentimiento/robertuito-sentiment-analysis"
"""

!pip install transformers
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Load the pre-trained model and tokenizer for sentiment analysis in Spanish
model_name = "finiteautomata/beto-sentiment-analysis"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Set the device to use for the model inference (either GPU or CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define a function to predict the sentiment score for a given text
def predict_sentiment(text):
    # Tokenize the input text and convert it to the appropriate format for the model input
    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    # Make the model prediction and convert the output to a probability distribution
    outputs = model(**inputs)
    probs = torch.softmax(outputs.logits, dim=1)
    # Extract the probability score for the positive sentiment class
    score = probs[0][2].item()
    return score

# Create new columns in the DataFrame to store the predicted sentiment scores
df["post_pandemic_sentiment"] = ""
df["next_summer_sentiment"] = ""

# Loop over each row in the DataFrame and predict the sentiment score for the corresponding text
for index, row in df.iterrows():
    tokens = row["post_pandemic"]
    # Convert the list of tokens to a string with whitespace-separated tokens
    text = " ".join(tokens)
    # Check if the input text is empty or contains only whitespace characters
    if not text.strip():
        df.at[index, "post_pandemic_sentiment"] = ""
    else:
        try:
            # Predict the sentiment score using the function
            score = predict_sentiment(text)
            # Assign the predicted sentiment score to the new column in the DataFrame
            df.at[index, "post_pandemic_sentiment"] = score
        except Exception as e:
            print(f"Error processing row {index}: {e}")
            df.at[index, "post_pandemic_sentiment"] = ""

    tokens = row["next_summer"]
    # Convert the list of tokens to a string with whitespace-separated tokens
    text = " ".join(tokens)
    # Check if the input text is empty or contains only whitespace characters
    if not text.strip():
        df.at[index, "next_summer_sentiment"] = ""
    else:
        try:
            # Predict the sentiment score using the function
            score = predict_sentiment(text)
            # Assign the predicted sentiment score to the new column in the DataFrame
            df.at[index, "next_summer_sentiment"] = score
        except Exception as e:
            print(f"Error processing row {index}: {e}")
            df.at[index, "next_summer_sentiment"] = ""

# Count the number of rows with each sentiment label 
sentiment_counts_post_pandemic = df["post_pandemic_sentiment"].value_counts()
sentiment_counts_next_summer = df["next_summer_sentiment"].value_counts()
# Print the sentiment counts
print("Post pandemic sentiment:")
print(sentiment_counts_post_pandemic)
print("- - - - - - - - - - - - - - -")
print("Next summer sentiment:")
print(sentiment_counts_next_summer)

print(df.shape)
print(df.columns)

"""## 2) WORD COUNT

Simple count of the amount of tokens per document.
"""

# Define a function to count the number of tokens in a list
def count_tokens(tokens):
    return len(tokens)

# Apply the function to the "post_pandemic" column to count the number of tokens in each row
df["post_pandemic_wordcount"] = df["post_pandemic"].apply(lambda x: count_tokens(x))

# Apply the function to the "next_summer" column to count the number of tokens in each row
df["next_summer_wordcount"] = df["next_summer"].apply(lambda x: count_tokens(x))

print("Word count of post_pandemic")
print(df['post_pandemic_wordcount'].describe())
print("- - - - - - -- - - - - - - - - -")
print("Word count of next_summer")
print(df['next_summer_wordcount'].describe())

print(df.shape)
print(df.columns)

"""## 3) LEXICON

Count of the amount of tokens from each ad hoc lexicon per document. 

Check: I do not think it is working fine, I think it does not take into account string with more than one word.
"""

# Define the lexicons
negativity = ["no", "nada", "solo", "sin", "nadie", "nunca", "ni", "jamas", "ni siquiera", "tampoco", "ningun", "mal", "feo", "peor", "poco", "tremendo", "terrible", "impactante", "deficiente", "pésimo", "inferior", "injusto", "cruel", "penoso", "difícil", "escaso", "insuficiente", "reducido", "ridículo", "irrisorio", "irritante", "desesperante", "insoportable", "horrible", "atroz", "aterrador", "duro", "brutal", "arduo", "complejo", "imposible", "lamentable", "aburrido", "triste", "preocupado", "estresado", "inquieto", "intranquilo", "nervioso", "ansioso", "deprimido", "agobiado", "desvelado", "desganado", "cansado", "fastidiado", "harto", "afligido", "apenado", "alarmado", "agotado", "angustiado"]
threat = ["peligro", "amenaza", "riesgo", "exponerse", "crisis", "grave", "miedo", "preocupación", "problema", "asustado", "drama", "temor", "inseguridad", "estrés", "presión", "muerte", "enfermedad", "intimidante", "maldición", "desafío", "advertencia", "fatalidad", "desgracia", "tragedia", "accidente", "dificultad", "ruina", "insomnio", "intimidado"]
uncertainty = ["no sé", "hay que ver", "quizás", "tal vez", "inesperado", "impredecible", "duda", "cambio", "vicisitud", "desequilibrio", "súbito", "repentino", "fortuito", "impensado", "desconcertante", "cambio brusco", "imprevisto", "insospechado", "repentino", "alterado", "variable", "desconfianza", "incertidumbre", "indeciso", "sospecha", "recelo", "perplejo", "vacilación", "azar", "inseguridad", "inquietud", "precaución", "cautela", "reticencia", "sin garantía"]
loneliness = ["falta de socializacion", "abandono", "incomodo", "solo", "solitario", "sin amigo", "ningun amigo", "apartado", "me evitan", "desolado", "por mi cuenta", "no puedo ver a nadie", "no me puedo ver mi", "sin compania", "abatido", "abadonado", "destituido", "triste", "infeliz", "extranado", "distanciado", "me siento ignorado", "encontrar a alguien", "sombrio", "oscuro", "sin hogar", "sin lugar", "no tengo a nadie", "extraño", "quiere ver a", "en una caja", "encerrado", "confinado", "me ignoran", "aislado", "aislamiento", "confinamiento", "confinado", "cuarentena", "soledad", "lugubre", "hacer amigos", "miserable", "malhumorado", "turbio", "sin amigos", "a nadie le importa", "a nadie le importo", "nadie con quien hablar", "perdida de contacto", "nadie me quiere", "nadie quiere conmigo", "nadie me extraña", "nadie me va a extrañar", "paria", "gente en mi vida", "reclusion", "rechazo", "rechazado", "renuncio", "recluido", "soltero", "hosco", "para salir", "juntarse", "atrapado", "troglodita", "extraterrestre", "nadie se ocupa de mi", "no me quieren", "no me aprecian", "antisocial", "quiero alguien para", "miserable", "defectuoso", "algo anda mal conmigo", "el problema soy yo", "no salir", "no juntarse", "no ver gente", "solo", "en casa", "encerrado", "apartado separado", "solitario", "incomunicado", "excluido", "confinado", "confinamiento", "vacio"]
depression = ["aburrimiento", "triste", "inútil", "no sirve", "solo", "encerrado", "aislado", "soledad", "perdida", "solo", "culpa", "merecer", "infeliz", "odio", "dolor", "inseguro", "inseguridad", "estres", "desganado", "cansado", "desanimado", "fastidio", "harto", "tedioso", "desesperante", "afligido", "apenado", "llanto", "llorar", "deplorable", "trágico", "inservible", "inepto", "incompetente", "vano", "estéril", "patético", "desafortunado", "mala suerte", "incapaz", "desconsuelo", "pesar", "suplicio", "deprimido", "tristeza", "vacío", "desesperanza", "pésimo", "pesimista", "pesimismo", "culpa", "cansado", "cansancio", "sin energía", "sin ganas", "no me puedo concentrar", "sin fuerza", "no tengo ganas", "ya no quiero nada", "no puedo dormir", "insomnio", "desvelo", "duermo mucho", "quiero estar todo el dia en la cama", "me despierto durante la noche", "no me puedo levantar", "pegado a la cama", "sin ganas de comer", "comer poco", "no quiero comer", "no tengo hambre", "como mucho", "como todo el dia", "solo quiero comer y dormir", "irritable", "malhumorado", "con malhumor", "mala onda"]
nointerest = ["sin ganas", "que se yo", "me da igual", "no importa", "no quiero", "todo igual", "para qué", "vacío", "sin sentido", "abulia", "indiferencia", "no hay esperanza", "perdido", "despreocupado", "vano", "insensible", "desinteresado", "desesperanza", "cansado", "aburrido", "desconectado"]

# Define a function to count the number of words in a list that appear in a given lexicon
def count_lexicon_words(tokens, lexicon):
    count = 0
    for token in tokens:
        if token in lexicon:
            count += 1
    return count

# Loop over each lexicon and each column to count the number of words in the lexicon that appear in each row
for lexicon_name, lexicon in [("negativity", negativity), ("threat", threat), ("uncertainty", uncertainty), ("loneliness", loneliness), ("depression", depression), ("nointerest", nointerest)]:
    for column_name in ["post_pandemic", "next_summer"]:
        new_column_name = f"{lexicon_name}_{column_name}"
        df[new_column_name] = df[column_name].apply(lambda x: count_lexicon_words(x, lexicon))

print(df.shape)
print(df.columns)

# Print a description of all the new columns
new_columns= ['negativity_post_pandemic','negativity_next_summer', 
              'threat_post_pandemic', 'threat_next_summer',
              'uncertainty_post_pandemic', 'uncertainty_next_summer',
              'loneliness_post_pandemic', 'loneliness_next_summer',
              'depression_post_pandemic', 'depression_next_summer',
              'nointerest_post_pandemic', 'nointerest_next_summer']

for column in new_columns:
  print(column)
  print(df[column].describe())
  print("- - - - - -- - - - - - - - - -")

"""## 4) CONCRETENESS

To perform this part I will use the BCBL dictionary.

First I create a file with all the unique tokens. Through the BCBL website I get a dictionary with the concreteness of most of my words. 
Unfortunately all the words were not found: 

FOUND: 7927 MATCHES

UNKNOWN: 2271 WORDS
"""

# Create a list of all the unique tokens in the "post_pandemic" and "next_summer" columns
all_tokens = set()
for _, row in df.iterrows():
    all_tokens.update(row["post_pandemic"])
    all_tokens.update(row["next_summer"])

# Write the list of tokens to a file
with open("unique_tokens.txt", "w") as f:
    f.write("\n".join(all_tokens))

# Download the file from Google Colab
from google.colab import files
files.download("unique_tokens.txt")

"""CHECK: I think tokenization is not being done properly. """

filepath_concreteness = '/content/drive/MyDrive/NEGATIVE BIAS/FILES FOR FEATURE EXTRACTION/written_es_wordlist_out.txt'
df_concreteness = pd.read_csv(filepath_concreteness, sep='\t')
# remove third empty column
df_concreteness = df_concreteness.drop('Unnamed: 2', axis=1)
# remove rows with Nan value
df_concreteness = df_concreteness.dropna(subset=['concreteness'])

df_concreteness

def add_concreteness_values(df, df_concreteness):
    # create a dictionary to map words to concreteness values
    concreteness_dict = dict(zip(df_concreteness['word'], df_concreteness['concreteness']))

    # iterate over rows in df
    concreteness_post_pandemic = []
    concreteness_next_summer = []
    for idx, row in df.iterrows():
        # get list of tokens in "post_pandemic" column
        tokens_post_pandemic = row['post_pandemic']
        # lookup concreteness values for tokens and sum them
        concreteness_value_post_pandemic = sum([concreteness_dict.get(token, 0) for token in tokens_post_pandemic])
        # append concreteness value to list
        concreteness_post_pandemic.append(concreteness_value_post_pandemic)

        # get list of tokens in "next_summer" column
        tokens_next_summer = row['next_summer']
        # lookup concreteness values for tokens and sum them
        concreteness_value_next_summer = sum([concreteness_dict.get(token, 0) for token in tokens_next_summer])
        # append concreteness value to list
        concreteness_next_summer.append(concreteness_value_next_summer)

    # append lists as new columns to df
    df['concreteness_value_post_pandemic'] = concreteness_post_pandemic
    df['concreteness_value_next_summer'] = concreteness_next_summer

    return df

# call function to add concreteness values as new columns to df
df = add_concreteness_values(df, df_concreteness)

print("Concreteness value post pandemic: ")
print(df['concreteness_value_post_pandemic'].describe())
print("- - - - - - -- - - - - - ")
print("Concreteness value next summer: ")
print(print(df['concreteness_value_next_summer'].describe()))

print(df.columns)
print(df.shape)

"""## 5) POS TAGGING"""

import nltk

def pos_tag_count(df, column):
    # Create a list of all possible parts of speech
    pos_tags = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']

    # Create a new dataframe to store the pos tag counts
    pos_df = pd.DataFrame(columns=pos_tags)

    # Loop through each row of the dataframe
    for i in range(len(df)):
        # Get the list of tokens or string for the current row
        tokens = df.loc[i, column]
        if isinstance(tokens, str):
            tokens = nltk.word_tokenize(tokens)

        # Use nltk's pos_tag function to get the parts of speech for each token
        pos_list = nltk.pos_tag(tokens)

        # Create a dictionary to store the counts for each pos tag
        pos_counts = {}
        for pos in pos_tags:
            pos_counts[pos] = 0

        # Loop through each pos tag in the pos_list and update the counts in the pos_counts dictionary
        for pos in pos_list:
            if pos[1] in pos_counts:
                pos_counts[pos[1]] += 1

        # Append the pos_counts dictionary as a new row in the pos_df dataframe
        pos_df = pos_df.append(pos_counts, ignore_index=True)

    # Rename the columns to include the original column name and the pos tag
    new_columns = []
    for col in pos_df.columns:
        new_col = column + '_' + col
        new_columns.append(new_col)
    pos_df.columns = new_columns

    # Concatenate the pos_df dataframe with the original dataframe
    df = pd.concat([df, pos_df], axis=1)

    return df

df = pos_tag_count(df, "post_pandemic")
df = pos_tag_count(df, "next_summer")

"""# Save pre-process data"""

# Save df

# Define the path to the folder where you want to save the file
folder_path = '/content/drive/MyDrive/NEGATIVE BIAS/DATA SETS'

# Define the path to the output file
output_file = folder_path + 'pre_processed_data.csv'

# Save the dataframe to a CSV file
df.to_csv(output_file, index=False)

#Summary of the statistics 
df.describe().round(1).T

"""# *4 - PREPARE SPLIT*

## New load of pre-processed data
"""

pre_df = pd.read_csv('/content/drive/MyDrive/NEGATIVE BIAS/DATA SETS/pre_processed_data.csv')

df_final= pre_df.drop(['post_pandemic', 'next_summer'], axis=1)
df_f= df_final.dropna()

"""## Split: train, dev and test sets. """

from sklearn.model_selection import train_test_split

# Split the DataFrame into temp 80% (training 80%, development 20%) and test 20%
temp_df, test_df = train_test_split(df_f, test_size=0.2, random_state=42)
train_df, dev_df = train_test_split(temp_df, test_size=0.2, random_state=42)

# Print the number of examples in each set
print('Number of examples in train set:', len(train_df))
print('Number of examples in dev set:', len(dev_df))
print('Number of examples in test set:', len(test_df))

# Split the input features (X) and target variables (y) for each set
X_train = train_df.drop(['depression', 'anxiety'], axis=1)
y_train_depression = train_df['depression']
y_train_anxiety = train_df['anxiety']

X_dev = dev_df.drop(['depression', 'anxiety'], axis=1)
y_dev_depression = dev_df['depression']
y_dev_anxiety = dev_df['anxiety']

X_test = test_df.drop(['depression', 'anxiety'], axis=1)
y_test_depression = test_df['depression']
y_test_anxiety = test_df['anxiety']

"""# 5 *-MODELS*

# Building, training and evaluation.
"""

from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

"""## 1) Linear regression with L2 penalty (linear)

For the linear regression model, the main hyperparameter tuned was the regularization strength, which is controlled by the alpha parameter. The alpha parameter determines the trade-off between fitting the training data and preventing overfitting. The goal of the hyperparameter tuning was to find the optimal value of alpha that minimizes the mean squared error on the training data while still generalizing well to the test data. In this case, a grid search was used to test different values of alpha and find the best value that resulted in the lowest mean squared error.
"""

# Define a Ridge regression model with L2 penalty
model_depression_lr = Ridge()
model_anxiety_lr = Ridge()

# Define the parameter grid for hyperparameter tuning
param_grid = {'model__alpha': [0.1, 1.0, 10.0]}

# Define a pipeline for feature scaling and modeling
pipeline_depression_lr = Pipeline([
    ('scaler', StandardScaler()),
    ('model', model_depression_lr)
])

pipeline_anxiety_lr = Pipeline([
    ('scaler', StandardScaler()),
    ('model', model_anxiety_lr)
])

# Perform grid search with cross-validation to find the best hyperparameters for the depression model
grid_depression = GridSearchCV(pipeline_depression_lr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
grid_depression.fit(X_train, y_train_depression)

# Perform grid search with cross-validation to find the best hyperparameters for the anxiety model
grid_anxiety = GridSearchCV(pipeline_anxiety_lr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
grid_anxiety.fit(X_train, y_train_anxiety)

# Print the best hyperparameters for the depression and anxiety models
print('Best hyperparameters for the depression model:', grid_depression.best_params_)
print('Best hyperparameters for the anxiety model:', grid_anxiety.best_params_)

# Train the depression and anxiety models with the best hyperparameters using the training set and validate on the development set
model_depression_lr = Ridge(alpha=grid_depression.best_params_['model__alpha'])
model_depression_lr.fit(X_train, y_train_depression)

model_anxiety_lr = Ridge(alpha=grid_anxiety.best_params_['model__alpha'])
model_anxiety_lr.fit(X_train, y_train_anxiety)

# Predict the depression and anxiety values for the test set using the trained models
y_test_depression_pred = model_depression_lr.predict(X_test)
y_test_anxiety_pred = model_anxiety_lr.predict(X_test)

# Evaluate the performance of the depression and anxiety models using the test set
rmse_depression_test = mean_squared_error(y_test_depression, y_test_depression_pred, squared=False)
r2_depression_test = r2_score(y_test_depression, y_test_depression_pred)

rmse_anxiety_test = mean_squared_error(y_test_anxiety, y_test_anxiety_pred, squared=False)
r2_anxiety_test = r2_score(y_test_anxiety, y_test_anxiety_pred)

# Print the RMSE and R2 scores for the depression and anxiety models on the test set
print('RMSE for depression on the test set:', rmse_depression_test)
print('R2 score for depression on the test set:', r2_depression_test)

print('RMSE for anxiety on the test set:', rmse_anxiety_test)
print('R2 score for anxiety on the test set:', r2_anxiety_test)

"""## 2) Random Forest (no-linear)

For the random forest model, the main hyperparameters tuned were the number of trees in the forest (n_estimators) and the maximum depth of the trees (max_depth). The n_estimators parameter controls the number of trees in the forest, which affects the variance of the model. The max_depth parameter controls the depth of the decision trees, which affects the bias-variance trade-off of the model. The goal of the hyperparameter tuning was to find the optimal combination of n_estimators and max_depth that maximizes the R2 score on the training data while still generalizing well to the test data. In this case, a grid search was used to test different combinations of n_estimators and max_depth and find the best combination that resulted in the highest R2 score.
"""

from sklearn.ensemble import RandomForestRegressor

# Define a random forest regression model
model_depression_rf = RandomForestRegressor()
model_anxiety_rf = RandomForestRegressor()

# Define the parameter grid for hyperparameter tuning
param_grid = {'model__n_estimators': [100, 200, 300],
              'model__max_depth': [10, 20, 30]}

# Define a pipeline for feature scaling and modeling
pipeline_depression_rf = Pipeline([
    ('scaler', StandardScaler()),
    ('model', model_depression_rf)
])

pipeline_anxiety_rf = Pipeline([
    ('scaler', StandardScaler()),
    ('model', model_anxiety_rf)
])

# Perform grid search with cross-validation to find the best hyperparameters for the depression model
grid_depression = GridSearchCV(pipeline_depression_rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
grid_depression.fit(X_train, y_train_depression)

# Perform grid search with cross-validation to find the best hyperparameters for the anxiety model
grid_anxiety = GridSearchCV(pipeline_anxiety_rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
grid_anxiety.fit(X_train, y_train_anxiety)

# Print the best hyperparameters for the depression and anxiety models
print('Best hyperparameters for the depression model:', grid_depression.best_params_)
print('Best hyperparameters for the anxiety model:', grid_anxiety.best_params_)

# Train the depression and anxiety models with the best hyperparameters using the training set and validate on the development set
model_depression_rf = RandomForestRegressor(n_estimators=grid_depression.best_params_['model__n_estimators'],
                                          max_depth=grid_depression.best_params_['model__max_depth'])
model_depression_rf.fit(X_train, y_train_depression)

model_anxiety_rf = RandomForestRegressor(n_estimators=grid_anxiety.best_params_['model__n_estimators'],
                                       max_depth=grid_anxiety.best_params_['model__max_depth'])
model_anxiety_rf.fit(X_train, y_train_anxiety)

# Predict the depression and anxiety values for the test set using the trained models
y_test_depression_pred = model_depression_rf.predict(X_test)
y_test_anxiety_pred = model_anxiety_rf.predict(X_test)

# Evaluate the performance of the depression and anxiety models using the test set
rmse_depression_test = mean_squared_error(y_test_depression, y_test_depression_pred, squared=False)
r2_depression_test = r2_score(y_test_depression, y_test_depression_pred)

rmse_anxiety_test = mean_squared_error(y_test_anxiety, y_test_anxiety_pred, squared=False)
r2_anxiety_test = r2_score(y_test_anxiety, y_test_anxiety_pred)

# Print the RMSE and R2 scores for the depression and anxiety models on the test set
print('RMSE for depression on the test set:', rmse_depression_test)
print('R2 score for depression on the test set:', r2_depression_test)

print('RMSE for anxiety on the test set:', rmse_anxiety_test)
print('R2 score for anxiety on the test set:', r2_anxiety_test)

"""# 6- *FEATURE IMPORTANCE*

# For Linear Regression
"""

# Get the coefficients of the depression and anxiety models
coef_depression = model_depression_lr.coef_
coef_anxiety = model_anxiety_lr.coef_

# Get the absolute values of the coefficients as feature importance scores
feat_imp_depression = abs(coef_depression)
feat_imp_anxiety = abs(coef_anxiety)

# Print the feature importance scores for the depression and anxiety models
print('Feature importance scores for the depression model:', feat_imp_depression)
print('Feature importance scores for the anxiety model:', feat_imp_anxiety)

# Get the coefficients of the depression and anxiety models
coef_depression = model_depression_lr.coef_
coef_anxiety = model_anxiety_lr.coef_

# Get the absolute values of the coefficients as feature importance scores
feat_imp_depression = abs(coef_depression)
feat_imp_anxiety = abs(coef_anxiety)

# Get the feature names
feature_names = X_train.columns

# Create a dictionary with feature names and importance scores for depression
feat_imp_depression_dict = dict(zip(feature_names, feat_imp_depression))

# Convert dictionary to a Pandas dataframe for depression
feat_imp_depression_df = pd.DataFrame.from_dict(feat_imp_depression_dict, orient='index', columns=['Importance Score'])

# Sort the dataframe by importance score in descending order for depression
feat_imp_depression_df = feat_imp_depression_df.sort_values(by='Importance Score', ascending=False)

# Create a bar plot of feature importance scores for depression
plt.figure(figsize=(10,6))
sns.barplot(x=feat_imp_depression_df['Importance Score'], y=feat_imp_depression_df.index, color='b')
plt.title('Feature Importance Scores for the Depression Model')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.show()

# Create a dictionary with feature names and importance scores for anxiety
feat_imp_anxiety_dict = dict(zip(feature_names, feat_imp_anxiety))

# Convert dictionary to a Pandas dataframe for anxiety
feat_imp_anxiety_df = pd.DataFrame.from_dict(feat_imp_anxiety_dict, orient='index', columns=['Importance Score'])

# Sort the dataframe by importance score in descending order for anxiety
feat_imp_anxiety_df = feat_imp_anxiety_df.sort_values(by='Importance Score', ascending=False)

# Create a bar plot of feature importance scores for anxiety
plt.figure(figsize=(10,6))
sns.barplot(x=feat_imp_anxiety_df['Importance Score'], y=feat_imp_anxiety_df.index, color='r')
plt.title('Feature Importance Scores for the Anxiety Model')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.show()

"""# For Random Forest

Random Forest models have a built-in feature importance metric that provides a measure of how much each feature contributes to the performance of the model. This is done by calculating the average reduction in impurity across all trees in the forest when a particular feature is used to make a split. Features that contribute more to the reduction in impurity are considered more important. Feature importance can be used to identify the most influential features in the model and to select a subset of features to improve model performance or reduce complexity. It is important to note that feature importance should be interpreted with caution, as it does not necessarily indicate causality and may be affected by collinearity and other factors.
"""

# Get feature importance for depression model
importance_depression = model_depression_rf.feature_importances_
features_depression = X_train.columns
df_importance_depression = pd.DataFrame({'feature': features_depression, 'importance': importance_depression})
df_importance_depression = df_importance_depression.sort_values('importance', ascending=False)

# Get feature importance for anxiety model
importance_anxiety = model_anxiety_rf.feature_importances_
features_anxiety = X_train.columns
df_importance_anxiety = pd.DataFrame({'feature': features_anxiety, 'importance': importance_anxiety})
df_importance_anxiety = df_importance_anxiety.sort_values('importance', ascending=False)

# Plot feature importance for depression and anxiety models
plt.figure(figsize=(10, 8))
plt.subplot(2, 1, 1)
sns.barplot(data=df_importance_depression, x='feature', y='importance')
plt.title('Feature Importance for Depression Model')
plt.xticks(rotation=90)

plt.subplot(2, 1, 2)
sns.barplot(data=df_importance_anxiety, x='feature', y='importance')
plt.title('Feature Importance for Anxiety Model')
plt.xticks(rotation=90)

plt.tight_layout()
plt.show()

"""# 7 *-DESCRIPTIVE STATISTICS*"""

import seaborn as sns
import matplotlib.pyplot as plt

"""# BOXPLOTS

Boxplots are useful for visualizing the distribution of numerical features in a dataset. The box in the plot represents the interquartile range (IQR), which contains the middle 50% of the data, and the whiskers represent the range of the data outside the IQR. The dots or asterisks that are sometimes seen in boxplots represent individual data points that fall outside the whiskers. By examining boxplots of different features, we can get a sense of their variability and potential outliers.
"""

# Plot boxplots for numerical features
sns.boxplot(data=df_f)
plt.xticks(rotation=90)
plt.title('Boxplot of Numerical Features')
plt.show()

# Select the numerical columns to plot
cols_to_plot_1 = ['mental_fatigue', 'risk_perception','lockdown_adherence', 'loneliness', 'intolerance_uncertainty']

# Plot boxplots for selected columns
sns.boxplot(data=df_f[cols_to_plot_1])
plt.xticks(rotation=90)
plt.title('Boxplot of Features from the paper')
plt.show()

# Select the numerical columns to plot
cols_to_plot_2 = ['post_pandemic_sentiment','next_summer_sentiment', 'post_pandemic_wordcount',
       'next_summer_wordcount', 'concreteness_value_post_pandemic', 'concreteness_value_next_summer']

# Plot boxplots for selected columns
sns.boxplot(data=df_f[cols_to_plot_2])
plt.xticks(rotation=90)
plt.title('Boxplot of my features (without lexicons)')
plt.show()

# Select the numerical columns to plot
cols_to_plot_2 = ['negativity_post_pandemic',
       'negativity_next_summer', 'threat_post_pandemic', 'threat_next_summer',
       'uncertainty_post_pandemic', 'uncertainty_next_summer',
       'loneliness_post_pandemic', 'loneliness_next_summer',
       'depression_post_pandemic', 'depression_next_summer',
       'nointerest_post_pandemic', 'nointerest_next_summer']

# Plot boxplots for selected columns
sns.boxplot(data=df_f[cols_to_plot_2])
plt.xticks(rotation=90)
plt.title('Boxplot of only lexicons)')
plt.show()

"""# HISTOGRAMS

Histograms are another way to visualize the distribution of a variable, but they show the frequency of different values rather than summarizing them into quartiles like boxplots. Histograms are useful for identifying the shape of a distribution, such as whether it is symmetric, skewed, or multimodal. By looking at histograms of the target variables ("depression" and "anxiety"), we can see how they are distributed across the range of possible values.
"""

# Plot histograms for target variables
sns.histplot(data=pre_df, x='depression')
plt.title('Histogram of Depression')
plt.show()

sns.histplot(data=pre_df, x='anxiety')
plt.title('Histogram of Anxiety')
plt.show()

"""# PAIRWISE

Pairwise scatterplots show the relationship between pairs of numerical features or between numerical features and the target variables. Each point in the plot represents a single data point, and its position along the x- and y-axes represents the values of the two variables being compared. By examining pairwise scatterplots, we can look for patterns or trends in the data, such as whether there is a linear relationship between two features or whether there are clusters of points with similar values.
"""

# Plot pairwise scatterplots (only with what they already had on the paper)
sns.pairplot(data=df_f, x_vars=['mental_fatigue', 'risk_perception',
       'lockdown_adherence', 'loneliness', 'intolerance_uncertainty'], y_vars=['depression', 'anxiety'])
plt.show()

# Plot pairwise scatterplots (only with the features that I have extracted without lexicons)
sns.pairplot(data=df_f, x_vars=['post_pandemic_sentiment','next_summer_sentiment', 'post_pandemic_wordcount',
       'next_summer_wordcount', 'concreteness_value_post_pandemic', 'concreteness_value_next_summer'], y_vars=['depression', 'anxiety'])
plt.show()

# Plot pairwise scatterplots (only lexicons)
sns.pairplot(data=df_f, x_vars=['negativity_post_pandemic',
       'negativity_next_summer', 'threat_post_pandemic', 'threat_next_summer',
       'uncertainty_post_pandemic', 'uncertainty_next_summer',
       'loneliness_post_pandemic', 'loneliness_next_summer',
       'depression_post_pandemic', 'depression_next_summer',
       'nointerest_post_pandemic', 'nointerest_next_summer'], y_vars=['depression', 'anxiety'])
plt.show()
# -*- coding: utf-8 -*-
"""NegativeBias (Version 1) . ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F0QFR8-IpreAYBi8WWaMBJh59y1G60La

# **Cognitive bias in depression and anxiety: NLP analysis.**

Team: Satrajit S. Ghosh, Daniel M. Low, Maria Paz Oliva, Fernando Torrente.

This work is based on a survey in which participants were asked about future events during COVID-19 confinement. Measures related to depression and anxiety of the subjects were taken. Using NLP techniques and ML, we expect to be able predict the levels of depression and anxiety of the participants from their speeches.

# *1 - DATA SET*

The following dataset was build based on a survey carried out by INECO.
3617 participants were asked about future events during the beginning of the COVID-19 confinement.
Measures related to the depression, anxiety and other behaivors of the subjects were taken.
The data set has the following columns of information: 

1- Depression level ( based on PHQ-9: 0 to 27)

2- Anxiety level (based on GAD-7: 0 to 21)

3- Mental fatigue level ( 0 to 5) check!!!!

4- Risk perception ( 0 to 10)

5- Lockdown adherence (0 to 10)

6- Loneliness (0 to 10)

7- Intolerance to uncertainty ( 0 to 5) check!!!  

8- Answer to the question: “¿Cómo te imaginas que cambiará tu vida después de 
la pandemia?

9- Answer to the question:  “¿Cómo te imaginás que podría ser tu próximo verano?"

## Upload and clean data-set.
"""

# Mount google drive
from google.colab import drive
drive.mount('/content/drive')

# Load data and create data frame

# 9 columns: depression, anxiaty, mental fatigue, risk perception, lockdown adherence, intolerance to uncertainty, post pandemic answer, next summer answer
# 3.617 rows 

import pandas as pd 
# Set colum names
names = ['depression', 'anxiety', 'mental_fatigue', 'risk_perception', 'lockdown_adherence', 'loneliness', 'intolerance_uncertainty', 'post_pandemic', 'next_summer' ]
# Load csv file
df = pd.read_csv('/content/drive/MyDrive/SESGO COGNITIVO/dataset-negativebias - Hoja 1.csv', names=names)
# Remove Nan answers
df = df.dropna()
# Viasualization
print("Df shape is:", df.shape)
print(df)

"""## Pre-processing

This pre-process intends to perform: lower case, remove punctuation, remove stop words and tokenization. The output is a list of tokens for each doc.

Check: did not perform lemmatization because it was not working well.
"""

import string
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer

def preprocess(df):
    # Lowercase text
    df["post_pandemic"] = df["post_pandemic"].str.lower()
    df["next_summer"] = df["next_summer"].str.lower()

    # Remove punctuation
    translator = str.maketrans('', '', string.punctuation)
    df["post_pandemic"] = df["post_pandemic"].str.translate(translator)
    df["next_summer"] = df["next_summer"].str.translate(translator)

    # Remove stopwords for Spanish
    stopwords_es = set(stopwords.words('spanish'))
    df["post_pandemic"] = df["post_pandemic"].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords_es]))
    df["next_summer"] = df["next_summer"].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords_es]))

    # Lemmatize in Spanish
    #stemmer = SnowballStemmer('spanish')
    #df["post_pandemic"] = df["post_pandemic"].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))
    #df["next_summer"] = df["next_summer"].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))

    # Tokenize
    df["post_pandemic"] = df["post_pandemic"].apply(lambda x: word_tokenize(x))
    df["next_summer"] = df["next_summer"].apply(lambda x: word_tokenize(x))

    return df

# Preprocess df
preprocess(df)

"""## Descriptive statistics

Summary of the values obtained for the following columns: 

1- Depression level ( 0 to 27)

2- Anxiety level ( 0 to 21)

3- Mental fatigue level ( 0 to 5) CHECK: 5 TO 25?

4- Risk perception ( 0 to 10) 

5- Lockdown adherence (0 to 10)

6- Loneliness (0 to 10)

7- Intolerance to uncertainty ( 0 to 5) CHECK: 12 to 60?
"""

#Summary of the statistics 
df.describe().round(1).T

"""## Word Cloud

I want to easily visualize the most common tokens in the "post_pandemic" and "next_summer" columns for rows with low and high depression levels ( less or more than 13) and low and high anxiaty level (less or more than 10).
"""

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Create a subset of the DataFrame for values of "depression" less than or equal to 13
df_low_depression = df[df["depression"] <= 13]

# Create a subset of the DataFrame for values of "depression" greater than 13
df_high_depression = df[df["depression"] > 13]

# Replace NaN values with empty string and convert float values to string
df_low_depression = df_low_depression.fillna("").astype(str)
df_high_depression = df_high_depression.fillna("").astype(str)

# Combine the "post_pandemic" and "next_summer" columns for each subset into a single string
text_low_depression = " ".join(df_low_depression["post_pandemic"].explode().tolist() + df_low_depression["next_summer"].explode().tolist())
text_high_depression = " ".join(df_high_depression["post_pandemic"].explode().tolist() + df_high_depression["next_summer"].explode().tolist())

# Create a WordCloud object for each subset
wc_low_depression = WordCloud().generate(text_low_depression)
wc_high_depression = WordCloud().generate(text_high_depression)

# Plot the WordCloud objects
plt.figure(figsize=(10,5))
plt.subplot(1,2,1)
plt.imshow(wc_low_depression, interpolation='bilinear')
plt.title("Post-pandemic and Next summer tokens for depression <= 13")
plt.axis("off")

plt.subplot(1,2,2)
plt.imshow(wc_high_depression, interpolation='bilinear')
plt.title("Post-pandemic and Next summer tokens for depression > 13")
plt.axis("off")

plt.show()

# Create a subset of the DataFrame for values of "anxiety" less than or equal to 13
df_low_anxiety = df[df["anxiety"] <= 10]

# Create a subset of the DataFrame for values of "anxiety" greater than 13
df_high_anxiety = df[df["anxiety"] > 10]

# Replace NaN values with empty string and convert float values to string
df_low_anxiety = df_low_anxiety.fillna("").astype(str)
df_high_anxiety = df_high_anxiety.fillna("").astype(str)

# Combine the "post_pandemic" and "next_summer" columns for each subset into a single string
text_low_anxiety = " ".join(df_low_anxiety["post_pandemic"].explode().tolist() + df_low_anxiety["next_summer"].explode().tolist())
text_high_anxiety = " ".join(df_high_anxiety["post_pandemic"].explode().tolist() + df_high_anxiety["next_summer"].explode().tolist())

# Create a WordCloud object for each subset
wc_low_anxiety = WordCloud().generate(text_low_anxiety)
wc_high_anxiety = WordCloud().generate(text_high_anxiety)

# Plot the WordCloud objects
plt.figure(figsize=(10,5))
plt.subplot(1,2,1)
plt.imshow(wc_low_anxiety, interpolation='bilinear')
plt.title("Post-pandemic and Next summer tokens for anxiety <= 10")
plt.axis("off")

plt.subplot(1,2,2)
plt.imshow(wc_high_anxiety, interpolation='bilinear')
plt.title("Post-pandemic and Next summer tokens for anxiety > 10")
plt.axis("off")

plt.show()

"""# 2 - *FEATURES EXTRACTION*

The following items will be computed to be used as features and added to the data set: 

1- Sentiment analysis: determine the attitude that the subjects have regarding their own future, establishing a gradual scale whose poles are: positive attitude and negative attitude.

2- Word count: count the amount of words used by each subject.

3- Lexicons: design lexical sets and count their appearance.

4- Concreteness: by having a concreteness value for each token, we can take the average concreteness of a document (removing the tokens that are not in the dictionary from the count). 

5- TF-IDF: determine the degree of importance of specific words in the corpus, observing which are the most important words. CHECK

6- BERT: Vectorize text using pre-trained models designed to capture general meaning. CHECK

## 1) SENTIMENT ANALYSIS

To perform sentiment analysis we will use the pipeline function from transformers. 

The classifier is a pre-trained bert model: "pysentimiento/robertuito-sentiment-analysis"
"""

!pip install transformers

from transformers import pipeline

# Load the sentiment analysis pipeline using the pre-trained model
classifier = pipeline("sentiment-analysis", model="pysentimiento/robertuito-sentiment-analysis", tokenizer="pysentimiento/robertuito-sentiment-analysis")

# Create a new column in the DataFrame to store the predicted sentiment values
df["post_pandemic_sentiment"] = ""

# Loop over each row in the DataFrame and predict the sentiment of the "post_pandemic" column
for index, row in df.iterrows():
    tokens = row["post_pandemic"]
    # Convert the list of tokens to a string with whitespace-separated tokens
    text = " ".join(tokens)
    # Check if the input text is empty or contains only whitespace characters
    if not text.strip():
        df.at[index, "post_pandemic_sentiment"] = ""
        continue
    try:
        # Predict the sentiment using the pipeline
        result = classifier(text)[0]
        # Assign the predicted sentiment label to the new column in the DataFrame
        df.at[index, "post_pandemic_sentiment"] = result["label"]
    except Exception as e:
        print(f"Error processing row {index}: {e}")
        df.at[index, "post_pandemic_sentiment"] = ""

# Create a new column in the DataFrame to store the predicted sentiment values
df["next_summer_sentiment"] = ""

# Loop over each row in the DataFrame and predict the sentiment of the "post_pandemic" column
for index, row in df.iterrows():
    tokens = row["next_summer"]
    # Convert the list of tokens to a string with whitespace-separated tokens
    text = " ".join(tokens)
    # Check if the input text is empty or contains only whitespace characters
    if not text.strip():
        df.at[index, "next_summer_sentiment"] = ""
        continue
    try:
        # Predict the sentiment using the pipeline
        result = classifier(text)[0]
        # Assign the predicted sentiment label to the new column in the DataFrame
        df.at[index, "next_summer_sentiment"] = result["label"]
    except Exception as e:
        print(f"Error processing row {index}: {e}")
        df.at[index, "next_summer_sentiment"] = ""

# Count the number of rows with each sentiment label 
sentiment_counts_post_pandemic = df["post_pandemic_sentiment"].value_counts()
sentiment_counts_next_summer = df["next_summer_sentiment"].value_counts()
# Print the sentiment counts
print("Post pandemic sentiment:")
print(sentiment_counts_post_pandemic)
print("- - - - - - - - - - - - - - -")
print("Next summer sentiment:")
print(sentiment_counts_next_summer)

"""CARFULL:

The following line gave error: (may want to remove or truncate input to a max sequence length)

POST PANDEMIC

Error processing row 1014: index out of range in self

Error processing row 1575: index out of range in self

Error processing row 1606: index out of range in self

Error processing row 2147: index out of range in self

Error processing row 2250: index out of range in self

Error processing row 2830: index out of range in self

Error processing row 3348: index out of range in self

NEXT SUMMER

Error processing row 2339: index out of range in self
"""

# Remove rows 
df = df.drop(index=1014)
df = df.drop(index=1575)
df = df.drop(index=1606)
df = df.drop(index=2147)
df = df.drop(index=2250)
df = df.drop(index=3348)
df = df.drop(index=2830)
df = df.drop(index=2339)

# Define a dictionary mapping for sentiment values
sentiment_mapping = {'NEG': 0, 'NEU': 0.5, 'POS': 1}

# Apply the sentiment mapping to the "post_pandemic_sentiment" and "next_summer_sentiment" columns
df['post_pandemic_sentiment'] = df['post_pandemic_sentiment'].map(sentiment_mapping)
df['next_summer_sentiment'] = df['next_summer_sentiment'].map(sentiment_mapping)

print(df['post_pandemic_sentiment'])

print(df.shape)
print(df.columns)

"""## 2) WORD COUNT

Simple count of the amount of tokens per document.
"""

# Define a function to count the number of tokens in a list
def count_tokens(tokens):
    return len(tokens)

# Apply the function to the "post_pandemic" column to count the number of tokens in each row
df["post_pandemic_wordcount"] = df["post_pandemic"].apply(lambda x: count_tokens(x))

# Apply the function to the "next_summer" column to count the number of tokens in each row
df["next_summer_wordcount"] = df["next_summer"].apply(lambda x: count_tokens(x))

print("Word count of post_pandemic")
print(df['post_pandemic_wordcount'].describe())
print("- - - - - - -- - - - - - - - - -")
print("Word count of next_summer")
print(df['next_summer_wordcount'].describe())

print(df.shape)
print(df.columns)

"""## 3) LEXICON

Count of the amount of tokens from each ad hoc lexicon per document. 

Check: I do not think it is working fine, I think it does not take into account string with more than one word.
"""

# Define the lexicons
negativity = ["no", "nada", "solo", "sin", "nadie", "nunca", "ni", "jamas", "ni siquiera", "tampoco", "ningun", "mal", "feo", "peor", "poco", "tremendo", "terrible", "impactante", "deficiente", "pésimo", "inferior", "injusto", "cruel", "penoso", "difícil", "escaso", "insuficiente", "reducido", "ridículo", "irrisorio", "irritante", "desesperante", "insoportable", "horrible", "atroz", "aterrador", "duro", "brutal", "arduo", "complejo", "imposible", "lamentable", "aburrido", "triste", "preocupado", "estresado", "inquieto", "intranquilo", "nervioso", "ansioso", "deprimido", "agobiado", "desvelado", "desganado", "cansado", "fastidiado", "harto", "afligido", "apenado", "alarmado", "agotado", "angustiado"]
threat = ["peligro", "amenaza", "riesgo", "exponerse", "crisis", "grave", "miedo", "preocupación", "problema", "asustado", "drama", "temor", "inseguridad", "estrés", "presión", "muerte", "enfermedad", "intimidante", "maldición", "desafío", "advertencia", "fatalidad", "desgracia", "tragedia", "accidente", "dificultad", "ruina", "insomnio", "intimidado"]
uncertainty = ["no sé", "hay que ver", "quizás", "tal vez", "inesperado", "impredecible", "duda", "cambio", "vicisitud", "desequilibrio", "súbito", "repentino", "fortuito", "impensado", "desconcertante", "cambio brusco", "imprevisto", "insospechado", "repentino", "alterado", "variable", "desconfianza", "incertidumbre", "indeciso", "sospecha", "recelo", "perplejo", "vacilación", "azar", "inseguridad", "inquietud", "precaución", "cautela", "reticencia", "sin garantía"]
#avoidance = []?
loneliness = ["falta de socializacion", "abandono", "incomodo", "solo", "solitario", "sin amigo", "ningun amigo", "apartado", "me evitan", "desolado", "por mi cuenta", "no puedo ver a nadie", "no me puedo ver mi", "sin compania", "abatido", "abadonado", "destituido", "triste", "infeliz", "extranado", "distanciado", "me siento ignorado", "encontrar a alguien", "sombrio", "oscuro", "sin hogar", "sin lugar", "no tengo a nadie", "extraño", "quiere ver a", "en una caja", "encerrado", "confinado", "me ignoran", "aislado", "aislamiento", "confinamiento", "confinado", "cuarentena", "soledad", "lugubre", "hacer amigos", "miserable", "malhumorado", "turbio", "sin amigos", "a nadie le importa", "a nadie le importo", "nadie con quien hablar", "perdida de contacto", "nadie me quiere", "nadie quiere conmigo", "nadie me extraña", "nadie me va a extrañar", "paria", "gente en mi vida", "reclusion", "rechazo", "rechazado", "renuncio", "recluido", "soltero", "hosco", "para salir", "juntarse", "atrapado", "troglodita", "extraterrestre", "nadie se ocupa de mi", "no me quieren", "no me aprecian", "antisocial", "quiero alguien para", "miserable", "defectuoso", "algo anda mal conmigo", "el problema soy yo", "no salir", "no juntarse", "no ver gente", "solo", "en casa", "encerrado", "apartado separado", "solitario", "incomunicado", "excluido", "confinado", "confinamiento", "vacio"]
depression = ["aburrimiento", "triste", "inútil", "no sirve", "solo", "encerrado", "aislado", "soledad", "perdida", "solo", "culpa", "merecer", "infeliz", "odio", "dolor", "inseguro", "inseguridad", "estres", "desganado", "cansado", "desanimado", "fastidio", "harto", "tedioso", "desesperante", "afligido", "apenado", "llanto", "llorar", "deplorable", "trágico", "inservible", "inepto", "incompetente", "vano", "estéril", "patético", "desafortunado", "mala suerte", "incapaz", "desconsuelo", "pesar", "suplicio", "deprimido", "tristeza", "vacío", "desesperanza", "pésimo", "pesimista", "pesimismo", "culpa", "cansado", "cansancio", "sin energía", "sin ganas", "no me puedo concentrar", "sin fuerza", "no tengo ganas", "ya no quiero nada", "no puedo dormir", "insomnio", "desvelo", "duermo mucho", "quiero estar todo el dia en la cama", "me despierto durante la noche", "no me puedo levantar", "pegado a la cama", "sin ganas de comer", "comer poco", "no quiero comer", "no tengo hambre", "como mucho", "como todo el dia", "solo quiero comer y dormir", "irritable", "malhumorado", "con malhumor", "mala onda"]
nointerest = ["sin ganas", "que se yo", "me da igual", "no importa", "no quiero", "todo igual", "para qué", "vacío", "sin sentido", "abulia", "indiferencia", "no hay esperanza", "perdido", "despreocupado", "vano", "insensible", "desinteresado", "desesperanza", "cansado", "aburrido", "desconectado"]

# Define a function to count the number of words in a list that appear in a given lexicon
def count_lexicon_words(tokens, lexicon):
    count = 0
    for token in tokens:
        if token in lexicon:
            count += 1
    return count

# Loop over each lexicon and each column to count the number of words in the lexicon that appear in each row
for lexicon_name, lexicon in [("negativity", negativity), ("threat", threat), ("uncertainty", uncertainty), ("loneliness", loneliness), ("depression", depression), ("nointerest", nointerest)]:
    for column_name in ["post_pandemic", "next_summer"]:
        new_column_name = f"{lexicon_name}_{column_name}"
        df[new_column_name] = df[column_name].apply(lambda x: count_lexicon_words(x, lexicon))

print(df.shape)
print(df.columns)

# Print a description of all the new columns
new_columns= ['negativity_post_pandemic','negativity_next_summer', 
              'threat_post_pandemic', 'threat_next_summer',
              'uncertainty_post_pandemic', 'uncertainty_next_summer',
              'loneliness_post_pandemic', 'loneliness_next_summer',
              'depression_post_pandemic', 'depression_next_summer',
              'nointerest_post_pandemic', 'nointerest_next_summer']

for column in new_columns:
  print(column)
  print(df[column].describe())
  print("- - - - - -- - - - - - - - - -")

"""## 4) CONCRETENESS

To perform this part I will use the BCBL dictionary.

First I create a file with all the unique tokens. Through the BCBL website I get a dictionary with the concreteness of most of my words. 
Unfortunately all the words were not found: 

FOUND: 7927 MATCHES

UNKNOWN: 2271 WORDS
"""

# Create a list of all the unique tokens in the "post_pandemic" and "next_summer" columns
all_tokens = set()
for _, row in df.iterrows():
    all_tokens.update(row["post_pandemic"])
    all_tokens.update(row["next_summer"])

# Write the list of tokens to a file
with open("unique_tokens.txt", "w") as f:
    f.write("\n".join(all_tokens))

# Download the file from Google Colab
from google.colab import files
files.download("unique_tokens.txt")

"""CHECK: I think tokenization is not being done properly. """

filepath_concreteness = '/content/drive/MyDrive/SESGO COGNITIVO/written_es_wordlist_out.txt'
df_concreteness = pd.read_csv(filepath_concreteness, sep='\t')
# remove third empty column
df_concreteness = df_concreteness.drop('Unnamed: 2', axis=1)
# remove rows with Nan value
df_concreteness = df_concreteness.dropna(subset=['concreteness'])

df_concreteness

def add_concreteness_values(df, df_concreteness):
    # create a dictionary to map words to concreteness values
    concreteness_dict = dict(zip(df_concreteness['word'], df_concreteness['concreteness']))

    # iterate over rows in df
    concreteness_post_pandemic = []
    concreteness_next_summer = []
    for idx, row in df.iterrows():
        # get list of tokens in "post_pandemic" column
        tokens_post_pandemic = row['post_pandemic']
        # lookup concreteness values for tokens and sum them
        concreteness_value_post_pandemic = sum([concreteness_dict.get(token, 0) for token in tokens_post_pandemic])
        # append concreteness value to list
        concreteness_post_pandemic.append(concreteness_value_post_pandemic)

        # get list of tokens in "next_summer" column
        tokens_next_summer = row['next_summer']
        # lookup concreteness values for tokens and sum them
        concreteness_value_next_summer = sum([concreteness_dict.get(token, 0) for token in tokens_next_summer])
        # append concreteness value to list
        concreteness_next_summer.append(concreteness_value_next_summer)

    # append lists as new columns to df
    df['concreteness_value_post_pandemic'] = concreteness_post_pandemic
    df['concreteness_value_next_summer'] = concreteness_next_summer

    return df

# call function to add concreteness values as new columns to df
df = add_concreteness_values(df, df_concreteness)

print("Concreteness value post pandemic: ")
print(df['concreteness_value_post_pandemic'].describe())
print("- - - - - - -- - - - - - ")
print("Concreteness value next summer: ")
print(print(df['concreteness_value_next_summer'].describe()))

print(df.columns)
print(df.shape)

"""## 5) TF-IDF : CHECK!!!

Now I will perform TF-IDF to convert text data into numerical features .
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# create TF-IDF vectorizer object
vectorizer = TfidfVectorizer()

# create TF-IDF vectorizer object
vectorizer = TfidfVectorizer()

# fit and transform "post_pandemic" and "next_summer" columns together
tfidf = vectorizer.fit_transform(df['post_pandemic'].apply(lambda x: ' '.join(x)) + ' ' + df['next_summer'].apply(lambda x: ' '.join(x)))

# convert sparse matrix to DataFrame
tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf)

# rename columns to feature names
tfidf_df.columns = ['tfidf_' + col for col in vectorizer.vocabulary_.keys()]

# concatenate TF-IDF DataFrame with original DataFrame
df_tfidf = pd.concat([df, tfidf_df], axis=1)

df_tfidf.shape

df_tfidf.columns

"""## 6) BERT: CHECK!!!"""

import torch
from transformers import BertTokenizer, BertModel

# Load pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertModel.from_pretrained('bert-base-multilingual-cased')

# Define a function to encode a text string as a dense vector using BERT
def encode_text(text):
    # Tokenize the text and add special tokens [CLS] and [SEP]
    input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])
    # Get the BERT model's hidden state for the input_ids
    with torch.no_grad():
        last_hidden_states = model(input_ids)[0]
    # Return the average of the BERT model's hidden states as a dense vector
    return torch.mean(last_hidden_states, dim=1).squeeze().tolist()

# Apply the encode_text function to the "post_pandemic" and "next_summer" columns
df['post_pandemic_bert'] = df['post_pandemic'].apply(lambda x: encode_text(' '.join(x)))
df['next_summer_bert'] = df['next_summer'].apply(lambda x: encode_text(' '.join(x)))

"""# 3 - *MODELS*

## Split: train, dev and test sets.
"""

from sklearn.model_selection import train_test_split

df_final = df_tfidf.drop(['post_pandemic', 'next_summer'], axis=1)
df_final= df_final.dropna()

# Split the DataFrame into training, development, and test sets
train_df, temp_df = train_test_split(df_final, test_size=0.4, random_state=42)
dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

# Print the number of examples in each set
print('Number of examples in train set:', len(train_df))
print('Number of examples in dev set:', len(dev_df))
print('Number of examples in test set:', len(test_df))

# Split the input features (X) and target variable (y) for each set
X_train = train_df.drop('depression', axis=1)
y_train = train_df['depression']
X_dev = dev_df.drop('depression', axis=1)
y_dev = dev_df['depression']
X_test = test_df.drop('depression', axis=1)
y_test = test_df['depression']

"""## Building - training - evaluation

## 1) Linear regression with L2 penalty (linear)
"""

# Train a Ridge regression model with L2 penalty using the training set
from sklearn.linear_model import Ridge
model_linear = Ridge(alpha=1.0)
model_linear.fit(X_train, y_train)

# Evaluate the performance of the model on the test set using mean squared error (MSE)
from sklearn.metrics import mean_squared_error
test_mse_linear = mean_squared_error(test_df['depression'], model_linear.predict(test_df.drop('depression', axis=1)))
print('Test Set MSE:', test_mse_linear)

"""## 2) Random Forest (no-linear)"""

from sklearn.ensemble import RandomForestRegressor

# Train a random forest model using the training set
model_rf = RandomForestRegressor(n_estimators=100, random_state=42)
model_rf.fit(X_train, y_train)

# Evaluate the performance of the model on the test set using mean squared error (MSE)
y_test_pred = model_rf.predict(X_test)
test_mse_rf = mean_squared_error(y_test, y_test_pred)
print('Test Set MSE:', test_mse_rf)